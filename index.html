<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Arm-Aware Guided Dexterous Grasp Generation with Arm-Agnostic Grasp Models">
  <meta name="keywords" content="">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Arm-Aware Guided Dexterous Grasp Generation with Arm-Agnostic Grasp Models</title>

  <!-- <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>



  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="./static/css/misc.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<section class="hero">
  <div class="hero-body">
    <div class="container is-fullhd">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Arm-Aware Guided Dexterous Grasp Generation <br />
                        with Arm-Agnostic Grasp Models </h1>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              Anonymous Author(s)
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="is-size-4 publication-authors">
              <span class="author-block"><b>Submitted to R-AL 2026</b></span>
            </div>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a target="_blank" href="./AdaptiGraph_RSS24.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

              <!-- Arxiv Link. -->
              <span class="link-block">
                <a target="_blank" href="docs/Text.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a target="_blank" href="docs/Appendix.pdf" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file"></i>
                  </span>
                  <span>Appendix</span>
                </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a target="_blank" href="https://arm-aware-dexgrasp.github.io/"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Comming soon)</span>
                </a>
              </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
          <video id="head" muted preload="metadata" height="90%" width="90%" playsinline autoplay loop >
            <source src="docs/videos/head.mp4" type="video/mp4">
          </video>

        <h2 class="subtitle has-text-centered">
          Real-world grasp execution requires considering <b>arm-related constraints</b>, 
          Conventional methods rely on rejection sampling to discard even near-feasible hand poses, leading 
          to low efficiency. We propose an <b>arm-aware dexterous grasp generation</b> approach that guides the pretrained 
          arm-agnostic model with arm-related constraints during denoising process, 
          greatly improving sampling efficiency.
        </h2>
      </div>
    </div>
  </div>
</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">

          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
            Dexterous grasp generation that considers arm-related constraints is crucial in real-world scenarios involving arm-environment collision avoidance, 
            workspace boundary grasps, and consecutive grasping. Existing hand-centric grasp models, which primarily focus on the floating hand’s pose, are insufficient 
            for such cases. Conventional arm-aware methods either rely on rejection sampling to discard infeasible samples or require retraining on arm-specific data, 
            leading to low sample efficiency under adverse conditions or limited generalization across different robots and environments. To overcome these limitations, 
            this letter presents an arm-aware dexterous grasp generation framework that leverages pretrained arm-agnostic grasp models while integrating arm and environmental
            information only at inference time. Specifically, we formulate arm-aware constrained grasp generation as a joint optimization of hand pose and arm configuration, 
            and derive closed-form gradients for arm-related constraints. Assuming the hand pose distribution is represented by a diffusion model, we prove that gradient-based 
            optimization is equivalent to guided diffusion sampling, steering near-feasible samples toward the feasible region. Through comprehensive evaluation involving 10k 
            objects across 6 scenarios, we demonstrate that the proposed framework generates feasible grasps in highly constrained settings with significantly higher probability, 
            highlighting its advantages in real-world applications.
            </p>
          </div>
          <!-- <br> -->

          <h2 class="title is-3">Video</h2>
          <video id="teaser" muted preload="metadata" height="100%" width="100%" controls="controls" playsinline>
            <source src="docs/videos/RAL25_Vid_compressed.mp4" type="video/mp4">
          </video>

        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">

          <h2 class="title is-3">Method Overview</h2>
          <div class="content has-text-justified">
            <p> Overview of the proposed arm-aware dexterous grasp generation method. Initially, we pretrain an arm-agnostic
              diffusion model to capture the distribution of wrist poses for floating hands. During sampling, arm kinematics
              and environment SDF are integrated as constraints, with their gradients guiding the denoising process. This approach
              significantly enhances the proportion of feasible grasps, adaptable to various arm-hand configurations and constrained environments.
            </p>
          </div>
          <div class="columns is-centered has-text-centered">
            <div class="column is-fullwidth">
              <div class="column is-fullwidth">
                <img src="docs/figs/pipeline.png" alt="" height="110%" width="110%">
              </div>
            </div>
          </div>
          <h3 class="title is-4 has-text-centered">Key Novelties</h3>

          <div class="content has-text-justified" style="margin-top: 20px;">
            <p>The key contributions and novelties of our approach beyond existing methods include:</p>
            <ol>
              <li>
                We formulate arm-aware grasp generation as a joint optimization of grasp pose and arm configuration, 
                deriving its relation to guided sampling on the pre-trained arm-agnostic grasp diffusion model with added arm constraints.
              </li> 
              <li>
                We derive analytical forms and gradients for three commonly used arm-related constraints (collision avoidance, hand reachability, 
                and joint proximity) to create the gradient for guidance, addressing the complex mapping between joint-space constraints and Cartesian-space denoising.
              </li>
              <li>
                We design comprehensive benchmark scenarios for simulation and real-world evaluation, featuring high obstacle coverage and grasps near arm limits, 
                which thoroughly verifies that our method generates successful grasps that satisfy constraints with a significantly higher probability than the commonly 
                used rejection sampling strategy. The proposed approach is applicable to various robotic arms (e.g., UR5 and Franka) and environments, utilizing a single 
                hand-centric grasp generation model.
              </li>
            </ol>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-widescreen">

        <h2 class="title is-3">Simulation Results</h2>
        <div class="content has-text-justified">
          <p style="text-align: center;">
            We validate the approach through large-scale simulations involving 15k grasps across 478 objects on Shadow,
            Allegro, and Leap Hands.
          </p>
        </div>

        <h3 class="title is-4">Evaluation Under Shape Uncertainty</h3>
        <div class="content has-text-justified">
          <p style="text-align: center;">
            Using single-view point clouds as observations of the grasp pose generation network. Some examples achieved
            using our method.
          </p>
        </div>
        <video id="sim_shape_diverse" autoplay muted playsinline height="70%" width="70%" controls
          style="display: block; margin: 0 auto; margin-bottom: 40px;">
          <source src="docs/videos/sim_shape_diverse.mp4" type="video/mp4">
        </video>

        <div>
          <p style="text-align: center;">Some examples of comparison with baselines. (Use the left and right buttons to
            switch between different cases)</p>
        </div>
        <div id="sim-shadow-dist0-section"></div>
        <div id="sim-allegro-dist0-section"></div>
        <div id="sim-leap_tac3d-dist0-section"></div>

        <h3 class="title is-4" style="margin-top: 40px;">Evaluation Under Position Uncertainty</h3>
        <div class="content has-text-justified">
          <p style="text-align: center;">
            The initial object positions are perturbed by 2 cm along eight uniformly distributed planar directions. Some
            examples achieved
            using our method.
          </p>
        </div>
        <video id="sim_pos_diverse" autoplay muted playsinline height="70%" width="70%" controls
          style="display: block; margin: 0 auto; margin-bottom: 40px;">
          <source src="docs/videos/sim_pos_diverse.mp4" type="video/mp4">
        </video>

        <div>
          <p style="text-align: center;">Some examples of comparison with baselines.</p>
        </div>
        <div id="sim-shadow-dist2-section"></div>
        <div id="sim-allegro-dist2-section"></div>
        <div id="sim-leap_tac3d-dist2-section"></div>

      </div>
    </div>
  </div>
</section>


<section class="hero">
  <div class="hero-body">
    <div class="columns is-centered has-text-centered">
      <div class="container is-max-widescreen">

        <h2 class="title is-3" style="margin-top: 40px;"> Real-World Experimental Results </h2>
        <div class="content has-text-justified">
          <p style="text-align: center;">
            Real-world experiments are conducted on a UR5 arm and a LEAP Hand. Each fingertip is equipped with a
            vision-based tactile sensor named Tac3D.
          </p>
        </div>

        <h3 class="title is-4" style="margin-top: 40px;">Evaluation Under Shape Uncertainty</h3>
        <div class="content has-text-justified">
          <p style="text-align: center;">
            Using single-view point clouds as observations of the grasp pose generation network. (Use the left and right
            buttons to switch between different cases)
          </p>
        </div>
        <div id="real-shape-section"></div>

        <h3 class="title is-4" style="margin-top: 40px;">Evaluation Under Position Uncertainty</h3>
        <div class="content has-text-justified">
          <p style="text-align: center;">
            We further conduct experiments under large position errors, using the glass vase and mosquito repellent
            bottle. The object is displaced by approximately 2 cm from its original position, either towards the thumb
            or index finger.
          </p>
        </div>
        <div id="real-pos-section"></div>

      </div>
    </div>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-centered">
          <p>
            Website template borrowed from <a href="https://robopil.github.io/adaptigraph/">AdaptiGraph</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<style>
  .grid-container {
    display: flex;
    justify-content: center;
    align-items: flex-start;
    gap: 20px;
    /* 视频之间的间距 */
    flex-wrap: nowrap;
    /* 若想小屏换行，改为 wrap */
    width: 100%;
  }

  .video-item {
    display: flex;
    flex-direction: column;
    align-items: center;
    flex: 1;
    /* 每个视频等宽自动分配空间 */
    max-width: 20%;
    /* 控制最大宽度，可调大些 */
  }

  .video-item video {
    width: 100%;
    height: auto;
    border-radius: 8px;
  }

  .video-item p {
    text-align: center;
    margin-top: 8px;
    font-size: 16px;
  }
</style>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    const sections = [
      { url: "sections/sim_res/shape_uncertainty/shadow.html", id: "sim-shadow-dist0-section", carouselId: "sim-shadow-dist0-results-carousel" },
      { url: "sections/sim_res/shape_uncertainty/allegro.html", id: "sim-allegro-dist0-section", carouselId: "sim-allegro-dist0-results-carousel" },
      { url: "sections/sim_res/shape_uncertainty/leap_tac3d.html", id: "sim-leap_tac3d-dist0-section", carouselId: "sim-leap_tac3d-dist0-results-carousel" },
      { url: "sections/sim_res/pos_uncertainty/shadow.html", id: "sim-shadow-dist2-section", carouselId: "sim-shadow-dist2-results-carousel" },
      { url: "sections/sim_res/pos_uncertainty/allegro.html", id: "sim-allegro-dist2-section", carouselId: "sim-allegro-dist2-results-carousel" },
      { url: "sections/sim_res/pos_uncertainty/leap_tac3d.html", id: "sim-leap_tac3d-dist2-section", carouselId: "sim-leap_tac3d-dist2-results-carousel" },
      { url: "sections/real_res/shape.html", id: "real-shape-section", carouselId: "real-shape-results-carousel" },
      { url: "sections/real_res/pos.html", id: "real-pos-section", carouselId: "real-pos-results-carousel" },
    ];
    Promise.all(
      sections.map(sec =>
        fetch(sec.url)
          .then(res => res.text())
          .then(html => {
            document.getElementById(sec.id).innerHTML = html;

            // Initialize this carousel
            const carousels = bulmaCarousel.attach(`#${sec.carouselId}`, {
              slidesToScroll: 1,
              slidesToShow: 1,
              loop: true,
              autoplay: false
            });

            const carousel = carousels[0];
            const slides = document.querySelectorAll(`#${sec.carouselId} .grid-container`);

            // Video autoplay control
            slides.forEach((slide, i) => {
              const vids = slide.querySelectorAll('video');
              vids.forEach(v => {
                if (i === 0) v.play();
                else {
                  v.pause();
                  v.currentTime = 0;
                }
              });
            });

            carousel.on('before:show', state => {
              slides.forEach((slide, i) => {
                const vids = slide.querySelectorAll('video');
                vids.forEach(v => {
                  if (i === state.next) v.play();
                  else {
                    v.pause();
                    v.currentTime = 0;
                  }
                });
              });
            });

          })
      )
    ).catch(err => console.error("Error loading sections:", err));
  });
</script>

</body> -->

</html>